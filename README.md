# PRN method
### "It's my first paper! I love it so much!"   
Let me give a simple introduction to the artical. With all my codes.   
Here's the `pdf` of the artical: [PRN-20240829.pdf](PRN-20240829.pdf)   
## Parallel relation network for fault detection of train transmission systems with zero-fault sample  
In this artical, we proposed such a method for Beijing PHM 2024 conference   
## Model structure   
This model mainl contains 3 part:   
### Residual Shrinkage Network  
  <p style="text-indent: 2em;">the **RSN** is designed to extraction the high-quality features by itself from time-frequency images. Unlike ResNets, RSN adds a soft threshold as a nonlinear adjustment layer. This layer effectively removes noise-related information, thereby extracting meaningful features.The soft threshold ensures that negative features are set to zero, preserving valuable features and attenuating unhelpful ones. This method effectively prevents gradient vanishing and exploding issues, as the derivative of the output is constrained to values of either 1 or 0.</p>   
### Feature Splicing   
  <p style="text-indent: 2em;">In order to describe the similarity of features between samples, we established the concept of sample pairs. In the feature concatenation module, features from different source samples are concatenated together as sample pairs. Health and anomaly samples are used to learn the unique characteristics while health samples are used to learn common characteristics. By analyzing the relationship between sample pairs, we can quantify the similarity between known and unknown samples. This helps to improve the accuracy of our fault detection under zero-sample conditions.</p>   
### Kolmogorov-Arnold Relation Networks   
  <p style="text-indent: 2em;">In the KARN module, we use **Kolmogorov-Arnold Networks (KAN)** to serve as relation networks. KAN is an innovative neural network architecture that proposes an alternative to the traditional multilayer perceptron (MLP). The core feature of KAN is that it places learnable activation functions at the edges of the network instead of at the traditional nodes, and each weight parameter is replaced by a univariate function, usually parameterized in the form of a spline function. This design not only improves the expressive power of the model, but also enhances its interpretability. KAN demonstrates excellent performance in tasks such as data fitting and partial differential equation solving, outperforming MLPs in accuracy and interpretability. In addition, KAN's neural scaling law shows the advantage of a faster decline of the test loss as the parameter is increased. The KAN's localized spline function also helps to avoid the catastrophic loss that occurs in the process of continuous learning. learning process, where catastrophic forgetting occurs. These features make KAN have great potential for application in scientific research and provide a new direction for the development of fault detection techniques.</p>   
  
![Model structure of PRN](figs/modelpre.png)
<p align="center"><strong>Model structure of PRN</span></p> 

## Auxiliary Sample Library (ASL)  
  An ASL is constructed to help extract high level features. Samples in this lib are generated by the soft Brownian offset method.  To know more about this method, you can read this artical: **[Out-of-distribution Detection and Generation using Soft Brownian Offset Sampling and Autoencoders](2105.02965)**
## Dataset  
Datasets are not included cause it's too large  
You can download it on conference website: https://www.icphm.org  

![Official Website for PHM 2024](pics/web.png)
<p align="center"><strong>Official Website for PHM 2024</span></p>  


### Actually those code sucks(not so clean), but they're truly useful



